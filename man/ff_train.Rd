% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ff_train.R
\name{ff_train}
\alias{ff_train}
\title{Train an XGBoost Model}
\usage{
ff_train(
  train_matrix,
  validation_matrix = NA,
  nrounds = 200,
  eta = 0.1,
  max_depth = 5,
  subsample = 0.75,
  eval_metric = "aucpr",
  early_stopping_rounds = 10,
  num_class = NULL,
  gamma = NULL,
  maximize = NULL,
  min_child_weight = 1,
  verbose = F,
  xgb_model = NULL,
  modelfilename = NULL,
  features = NULL,
  objective = "binary:logistic"
)
}
\arguments{
\item{train_matrix}{The training matrix for XGBoost. should be of type xgb.Dmatrix}

\item{validation_matrix}{The matrix to run for the model for XGBoost. should be of type xgb.Dmatrix}

\item{nrounds}{Number of boosting rounds. Default is 200.}

\item{eta}{Learning rate. Default is 0.1.}

\item{max_depth}{Maximum tree depth. Default is 5.}

\item{subsample}{Subsample ratio of the training instances. Default is 0.75.}

\item{eval_metric}{Evaluation metric. Default is "aucpr". This can also be a custom evaluation metric.}

\item{early_stopping_rounds}{Early stopping rounds. Default is 10.}

\item{gamma}{The gamma value, should be between 0 and 0.3. Determines level of pruning}

\item{maximize}{Default is NULL. Should be True or False in case a custom evaluation metric is used.}

\item{min_child_weight}{The minimum weight of the child, determines how quickly the tree grows}

\item{verbose}{should the model run verbose. Default is FALSE.}

\item{xgb_model}{Previous build model to continue the training from. Could be an object of class "xgb.Booster", its raw data, or a file name. Default = NULL}

\item{modelfilename}{character where to save the model. should end with the extension model}

\item{features}{Vector with the feature names of the training dataset. Should be given when modelfilename is given so that the next time the model is loaded the model knows which features were used}

\item{objective}{Specify the learning task and the corresponding learning objective. Default is "binary:logistic".}
}
\value{
Trained XGBoost model.
}
\description{
This function trains an XGBoost model with default parameters. The optimal parameters have been derived by running the algorithm worldwide on many tiles for over a year of training data.
}
\examples{
# Example usage:
train_matrix <- matrix(c(1, 2, 3, 4), ncol = 2)
model <- train_xgboost(train_matrix)

}
\references{
Jonas van Duijvenbode (2023)
Zillah Calle (2023)
}
\keyword{XGBoost}
\keyword{data}
\keyword{preparation}
