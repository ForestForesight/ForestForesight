% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ff_optimizer.R
\name{ff_optimizer}
\alias{ff_optimizer}
\title{Bayesian Optimization for XGBoost Hyperparameters with ForestForesight}
\usage{
ff_optimizer(
  ff_folder,
  shape = NULL,
  country = NULL,
  train_dates,
  val_dates = NULL,
  bounds = list(eta = c(0.01, 0.8), nrounds = c(10, 500), max_depth = c(2, 15), subsample
    = c(0.1, 0.6), gamma = c(0.01, 1), min_child_weight = c(1, 10)),
  init_points = 5,
  n_iter = 25,
  acq = "ucb",
  kappa = 2.576,
  ff_prep_params = list(),
  ff_train_params = list(),
  verbose = TRUE,
  validation_sample = 0.25
)
}
\arguments{
\item{ff_folder}{`character` The path to the folder containing the ForestForesight data.}

\item{shape}{`character` (optional) Shape parameter for data preparation, if required by `ff_prep`.}

\item{country}{`character` (optional) Country parameter for data preparation, if required by `ff_prep`.}

\item{train_dates}{`character` Dates for training data, formatted as required by `ff_prep`.}

\item{val_dates}{`character` Dates for validation data, formatted as required by `ff_prep`. If NULL, a portion of the training data will be used for validation.}

\item{bounds}{`list` List of hyperparameter bounds for Bayesian optimization. Default values:
\itemize{
  \item `eta`: Learning rate, range [0.01, 0.8]
  \item `nrounds`: Number of boosting rounds, range [10, 500]
  \item `max_depth`: Maximum depth of the trees, range [2, 15]
  \item `subsample`: Subsample ratio of the training instances, range [0.1, 0.7]
  \item `gamma`: Minimum loss reduction to make a further partition, range [0.01, 1]
  \item `min_child_weight`: Minimum sum of instance weight needed in a child, range [1, 10]
}}

\item{init_points}{`numeric` Number of initial random searches before starting Bayesian optimization. Default is 5.}

\item{n_iter}{`numeric` Number of iterations for the Bayesian optimization. Default is 25.}

\item{acq}{`character` Acquisition function to be used by the optimizer. Default is "ucb" (Upper Confidence Bound).}

\item{kappa}{`numeric` Exploration-exploitation parameter for the UCB acquisition function. Default is 2.576.}

\item{ff_prep_params}{`list` Additional parameters to be passed to the `ff_prep` function for data preparation.}

\item{ff_train_params}{`list` Additional parameters to be passed to the `ff_train` function for model training.}

\item{verbose}{`logical` Whether to print progress messages. Default is `TRUE`.}

\item{validation_sample}{`numeric` Proportion of training data to use for validation when `val_dates` is NULL. Default is 0.25.}
}
\value{
A list containing:
\itemize{
  \item `best_params`: The best hyperparameters found by the optimizer.
  \item `final_model`: The final trained model using the best hyperparameters.
  \item `optimization_result`: Full result of the Bayesian optimization process.
}
}
\description{
This function uses Bayesian optimization to tune the hyperparameters of an XGBoost model for a given dataset.
It leverages the `ForestForesight` package for data preparation and model training.
}
\details{
The function first prepares the training and validation datasets using the `ff_prep` function. If `val_dates` is NULL,
it uses random sampling to create a validation set from the training data based on the `validation_sample` parameter.

It then defines an objective function, `xgb_cv_bayes`, which trains the model with given hyperparameters and returns
the best score based on the validation set's AUCPR (Area Under the Precision-Recall Curve).

The `BayesianOptimization` function from the `rBayesianOptimization` package is used to find the optimal set of
hyperparameters within the specified bounds. Once the best parameters are found, the model is trained on the
entire training dataset, and the final model is returned.
}
\examples{
\dontrun{
# Optimize hyperparameters for a model
optimized_result <- ff_optimizer(
  ff_folder = "path/to/forestforesight/data",
  country = "BRA",
  train_dates = ForestForesight::daterange("2022-01-01", "2023-06-30"),
  val_dates = ForestForesight::daterange("2023-07-01", "2023-12-31"),
  bounds = list(eta = c(0.01, 0.2), nrounds = c(100, 300)),
  n_iter = 30,
  verbose = TRUE
)

# Use the optimized model
best_model <- optimized_result$final_model
}

}
\seealso{
\code{\link{ff_run}} for the main function to train and predict deforestation
\code{\link{ff_prep}} for data preparation
\code{\link{ff_train}} for model training
}
